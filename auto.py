# smart_crawler_auto.py

import re
import time
import csv
import os
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

import undetected_chromedriver as uc
from fake_useragent import UserAgent
from selenium_stealth import stealth
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# â”€â”€â”€ undetectedâ€chromedriver + stealth ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ua = UserAgent()
options = uc.ChromeOptions()
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument(f'user-agent={ua.random}')
options.add_argument('--start-maximized')
driver = uc.Chrome(options=options)
wait = WebDriverWait(driver, 10)
stealth(driver,
        languages=["ko-KR","ko"],
        vendor="Google Inc.",
        platform="Win32",
        webgl_vendor="Intel Inc.",
        renderer="Intel Iris OpenGL Engine",
        fix_hairline=True,
)

# â”€â”€â”€ 1. URL ì ‘ì† & ëŒ€í‘œ ìš”ì†Œ ì„ íƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
url = input("ğŸŒ í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ URL ì…ë ¥: ").strip()
if not url.startswith("http"):
    url = "https://" + url
driver.get(url)
time.sleep(2)

keyword = input("ğŸ” ìš”ì†Œ ê²€ìƒ‰ í‚¤ì›Œë“œ: ").strip()
cands = driver.find_elements(By.XPATH, f"//*[contains(text(), '{keyword}')]")
if not cands:
    print("âŒ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."); driver.quit(); exit()
print(f"{len(cands)}ê°œ í›„ë³´ ë°œê²¬:")
for i,e in enumerate(cands):
    txt = e.text.strip().replace('\n',' ')
    print(f"[{i}] <{e.tag_name}> â–¶ {txt[:40]}")
idx = int(input("ì„ íƒí•  ìš”ì†Œ ë²ˆí˜¸: "))
target = cands[idx]

# â”€â”€â”€ 2. CSS selector ìë™ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
selector = driver.execute_script("""
  function getCssSelector(el) {
    let path = [];
    while(el.nodeType===1 && el.tagName.toLowerCase()!=='html') {
      let sel = el.tagName.toLowerCase();
      if(el.className) sel += '.'+el.className.trim().split(/\\s+/)[0];
      path.unshift(sel);
      el=el.parentNode;
    }
    return path.join(' > ');
  }
  return getCssSelector(arguments[0]);
""", target)
os.makedirs("crawl_result", exist_ok=True)
with open("crawl_result/selector.txt","w",encoding="utf-8") as f:
    f.write(selector)
print("â†’ ìˆ˜ì§‘ ëŒ€ìƒ selector:", selector)

# â”€â”€â”€ 3. ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mc = input("ğŸ“¦ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ì—”í„°=ë¬´ì œí•œ): ").strip()
max_count = int(mc) if mc.isdigit() else None

# â”€â”€â”€ 4. ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
collected = set()
def collect_page():
    for el in driver.find_elements(By.CSS_SELECTOR, selector):
        t = el.text.strip()
        if t and t not in collected:
            collected.add(t)
            print("âœ…", t[:60])
            if max_count and len(collected)>=max_count:
                return True
    return False

# â”€â”€â”€ 5. í˜ì´ì§€ ìˆœíšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
page = 1
while True:
    print(f"\nğŸ“„ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘â€¦")
    if collect_page():
        break

    # 5-1. <link rel="next">
    try:
        href = driver.find_element(By.CSS_SELECTOR,'link[rel="next"]').get_attribute('href')
        driver.get(href); page+=1; time.sleep(2); continue
    except: pass

    # 5-2. a[rel="next"]
    try:
        btn = driver.find_element(By.CSS_SELECTOR,'a[rel="next"]')
        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,'a[rel="next"]')))
        btn.click(); page+=1; time.sleep(2); continue
    except: pass

    # 5-3. Google-specific: id="pnnext"
    try:
        gnext = driver.find_element(By.ID, "pnnext")
        driver.execute_script("arguments[0].scrollIntoView();", gnext)
        gnext.click(); page+=1; time.sleep(2); continue
    except: pass

    # 5-4. li.next > a (quotes.toscrape.com ë“±)
    try:
        nxt = driver.find_element(By.CSS_SELECTOR, 'li.next > a')
        if nxt.is_displayed():
            nxt.click(); page+=1; time.sleep(2); continue
    except: pass

    # 5-5. â€œë‹¤ìŒâ€ í…ìŠ¤íŠ¸/í´ë˜ìŠ¤ ê¸°ë°˜ ë²„íŠ¼
    clicked = False
    for xp in ['//a[normalize-space()="ë‹¤ìŒ"]','//button[normalize-space()="ë‹¤ìŒ"]','//a[contains(@class,"next")]']:
        try:
            nb = driver.find_element(By.XPATH, xp)
            if nb.is_displayed():
                driver.execute_script("arguments[0].scrollIntoView();", nb)
                try: nb.click()
                except: driver.execute_script("arguments[0].click();", nb)
                page+=1; time.sleep(2); clicked=True; break
        except: pass
    if clicked: continue

    # 5-6. ìˆ«ì í˜ì´ì§• ìë™ íƒì§€
    anchors = driver.find_elements(By.TAG_NAME,'a')
    nums = [(int(a.text), a) for a in anchors if a.text.isdigit()]
    if len(nums)>=2:
        nums.sort(key=lambda x:x[0])
        for num,a in nums:
            if num == page+1:
                driver.execute_script("arguments[0].scrollIntoView();", a)
                try: a.click()
                except: driver.execute_script("arguments[0].click();", a)
                page=num; time.sleep(2); clicked=True; break
    if clicked: continue

    # 5-7. URL íŒŒë¼ë¯¸í„° ê¸°ë°˜ page+1
    p = urlparse(driver.current_url)
    qs = parse_qs(p.query, keep_blank_values=True)
    bumped = False
    for k,v in qs.items():
        if re.search(r'page|pg|p$|start|offset', k, re.IGNORECASE) and v and v[0].isdigit():
            qs[k] = [str(int(v[0])+1)]
            newq = urlencode(qs, doseq=True)
            next_url = urlunparse((p.scheme,p.netloc,p.path,p.params,newq,p.fragment))
            driver.get(next_url); page+=1; time.sleep(2); bumped=True; break
    if bumped: continue

    # 5-8. ë¬´í•œ ìŠ¤í¬ë¡¤ fallback
    print("ğŸ”½ í˜ì´ì§• íƒì§€ ì‹¤íŒ¨ â†’ ë¬´í•œ ìŠ¤í¬ë¡¤")
    last_h = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
        time.sleep(2)
        if collect_page(): break
        new_h = driver.execute_script("return document.body.scrollHeight")
        if new_h==last_h or (max_count and len(collected)>=max_count):
            print("â›” ë” ì´ìƒ ë¡œë“œí•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤."); break
        last_h=new_h
    break

# â”€â”€â”€ 6. ê²°ê³¼ CSV ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
out = f"crawl_result/result_{int(time.time())}.csv"
with open(out,'w',encoding="utf-8",newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["ë‚´ìš©"])
    for t in sorted(collected):
        writer.writerow([t])

print(f"\nâœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected)}ê°œ")
print("ğŸ“ ì €ì¥ ê²½ë¡œ:", out)
driver.quit()
