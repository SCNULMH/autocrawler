# auto_crawler.py
# ì˜¤í† í¬ë¡¤ëŸ¬ í”„ë¡œê·¸ë¨ (ì‚¬ìš©ì í´ë¦­ ê¸°ë¡ + ì¬ìƒ ê¸°ëŠ¥ ì¶”ê°€)

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from tqdm import tqdm
import time
import csv
import os
import json

# 1. í¬ë¡¬ë“œë¼ì´ë²„ ê²½ë¡œ ì„¤ì •
service = Service(executable_path="./chromedriver.exe")
chrome_options = Options()
chrome_options.add_argument("--start-maximized")
driver = webdriver.Chrome(service=service, options=chrome_options)

# 2. URL ì…ë ¥ ë° ìë™ ë³´ì •
url = input("í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ URL ì…ë ¥: ")
if not url.startswith("http"):
    url = "https://" + url
print(f"âœ… ì ‘ì†í•  URL: {url}")
driver.get(url)

time.sleep(2)

# 3. í´ë¦­ ê¸°ë¡ ì—¬ë¶€
record_clicks = input("ğŸ’¬ í´ë¦­í•˜ëŠ” ë²„íŠ¼ì„ ê¸°ë¡í• ê¹Œìš”? (y/n): ").lower() == 'y'

click_history = []

if record_clicks:
    print("ğŸ–±ï¸ í´ë¦­í•˜ê³  ì‹¶ì€ ìš”ì†Œë¥¼ í´ë¦­í•˜ì„¸ìš”. (10ì´ˆ ë™ì•ˆ ê¸°ë¡)")
    start_time = time.time()
    while time.time() - start_time < 10:
        elements = driver.find_elements(By.XPATH, "//*")
        for elem in elements:
            try:
                if elem.is_displayed():
                    driver.execute_script("""
                    arguments[0].addEventListener('click', function(){
                        console.log('CLICKED:', arguments[0]);
                    });
                    """, elem)
            except:
                pass
        time.sleep(1)
    print("ğŸ›‘ í´ë¦­ ê¸°ë¡ ì‹œê°„ ì¢…ë£Œ")
else:
    # ê¸°ë¡ëœ í´ë¦­ íˆìŠ¤í† ë¦¬ê°€ ìˆë‹¤ë©´ ì¬ìƒ
    if os.path.exists('click_history.json'):
        with open('click_history.json', 'r', encoding='utf-8') as f:
            click_history = json.load(f)

    # ê¸°ë¡ëœ í´ë¦­ ì¬ìƒ
    for click in click_history:
        try:
            elem = driver.find_element(By.XPATH, click["xpath"])
            elem.click()
            print(f"âœ… ê¸°ë¡ëœ í´ë¦­ ìˆ˜í–‰: {click['xpath']}")
            time.sleep(1)
        except Exception as e:
            print(f"âŒ í´ë¦­ ì‹¤íŒ¨: {click['xpath']}, ì˜¤ë¥˜: {e}")

# 4. í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
html = driver.page_source
soup = BeautifulSoup(html, "html.parser")

# 5. ìˆ˜ì§‘í•  ë°ì´í„° ì¡°ê±´ ì…ë ¥
print("ğŸ¯ ìˆ˜ì§‘í•  ë°ì´í„° ì„¤ì •")
tag_name = input("ìˆ˜ì§‘í•  íƒœê·¸ ì´ë¦„ ì…ë ¥ (ì˜ˆ: div, span, a): ").strip()
class_name = input("í•„ìš”í•œ ê²½ìš° í´ë˜ìŠ¤ëª… ì…ë ¥ (ì—†ìœ¼ë©´ ì—”í„°): ").strip()

# 6. ë°ì´í„° ìˆ˜ì§‘
print("âœ… ë°ì´í„° í•„í„°ë§ ì‹œì‘...")

if class_name:
    items = soup.find_all(tag_name, class_=class_name)
else:
    items = soup.find_all(tag_name)

results = []
for item in tqdm(items, desc="ë°ì´í„° ìˆ˜ì§‘ ì¤‘"):
    text = item.get_text(strip=True)
    results.append([text])

# 7. ì €ì¥ í´ë”/íŒŒì¼ ì„¤ì •
save_folder = "crawl_result"
save_file = "result.csv"
os.makedirs(save_folder, exist_ok=True)

# 8. ê²°ê³¼ ì €ì¥ (CSV)
with open(os.path.join(save_folder, save_file), mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(["Content"])  # CSV í—¤ë”
    writer.writerows(results)

print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ ë° ì €ì¥ ì™„ë£Œ! (ê²½ë¡œ: {save_folder}/{save_file})")

# 9. í´ë¦­ ê¸°ë¡ ì €ì¥
if record_clicks:
    # (ì„ì‹œë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì €ì¥)
    with open('click_history.json', 'w', encoding='utf-8') as f:
        json.dump(click_history, f, indent=4, ensure_ascii=False)
    print("âœ… í´ë¦­ ê¸°ë¡ ì €ì¥ ì™„ë£Œ (click_history.json)")

# 10. ë¸Œë¼ìš°ì € ì¢…ë£Œ
driver.quit()
